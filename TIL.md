# Part1. Bagging & Boosting

## Ch1. Gradient Boosting in ML

XGBoost는 표 형식 데이터를 활용한 예측에서 가장 뛰어난 머신러닝 알고리즘이다.

부스팅의 일반적인 아이디어: 약한 학습기(weak learner)를 반복적으로 오차를 개선하여 강한 학습기로 바꾸는 것

Gradient Boosting의 핵심 아이디어: 경사 하강법을 사용해 잔여 오차를 최소화 하는 것

`df_bikes.isna().any(axis=1)`: 열을 따라 누락된 값이 하나 이상인 모든 행을 찾음

`df_bikes.iloc[[56, 81]]`: 두개 이상의 인덱스를 찾을 때는 대괄호 두번 사용

`df_bikes.groupby('season')['hum'].transform('median')`: transform()메서드는 첫번째 매개 변수로 전달된 함수를 적용한 다음 원본과 동일한 길이의 시리즈나 데이터프레임을 반환함

`df_bikes['dteday'].apply(pd.to_datetime, infer_datetime_format=True, errors='coerce')`: infer_datetime_format=True로 지정하면 판다스가 datetime 객체의 종류를 결정하며, 대부분의 경우 안전함

`df_bikes.loc[730, 'yr'] = 1.0`loc 메서드를 사용하여 행과 열을 지정하여 특정 값을 변경할 수 있음

`fillna` 메서드에 원본과 동일한 길이의 시리즈 객체를 전달하면 누락된 위치에 있는 값만 채우는데 사용함

사이킷런의 `cross_val_score` 함수의 `scoring='neg_mean_squared_error'` 라고 쓰는 이유: 사이킷런은 점수가 높은것을 좋은 것으로 판단하기 때문

`df.info()` 는 메모리 사용량도 확인시켜줌



## Ch2. Decision Tree

gini impurity는 낮을수록 분류가 잘 된 것. 0.5인 경우 무작위 추측보다 더 낫다고 볼 수 없음

딱 한번만 분할된 트리는 stump라고 부른다. 이 스텀프도 부스터로 사용되면 강력해질 수 있음

훈련된 reg 객체의 tree_ 속성에 훈련된 트리 객체가 저장되어 있음

​	children_left, children_right 속성은 자식 노드의 인덱스를 담고 있음. 따라서 이들이 -1이면 리프노드라는 의미

GridSearchCV의 핵심은 매개변수 값의 딕셔너리를 만드는 것

`max_features` 옵션

- 0~1: 전체 feature의 비율
- 정수: 사용할 feature 개수
- None, auto: 전체 feature
- sort: 전체 feature의 제곱근
- log2: 전체 feature의 로그(밑이2)

`splitter` 옵션을 'random'으로 하면 과대적합을 막고 다양한 트리를 만들 수 있음. 'best' 옵션은 정보 이득이 가장 큰 특성만을 선택함

회귀와 분류의 criterion은 다름

- 회귀: squared_error, friedman_mse, absolute_error, poisson
- 분류: gini, entropy

`RandomizedSearchCV` 는 모든 조합을 테스트하는 대신 랜덤한 조합을 테스트함. 제한된 시간 내에 최적의 조합을 찾게 해줌. 또한 분포를 이용하여 연속적 매개변수 샘플링 가능. randint, uniform, loguniform 등

모델을 결정한 후에는 테스트셋을 포함하여 모델을 훈련하는것이 정확도를 더 높일 수 있음. 단 모델을 실전에 투여했을 때 얻을 수 있는 성능을 추정하기 어려워짐

`operator.itemgetter` 는 sorted 같은 함수의 key 매개변수에 적용하여 다양한 기준으로 정렬할 수 있도록 하는 모듈

사이킷런에서 추천하는 특성 중요도 측정 방법은 `permutation_importance()`함수임. 이 함수는 기존 모델에 특성 하나를 랜덤하게 섞은 후 모델을 훈련하여 성능을 비교



## Ch3. Bagging & Randomforest

RF는 XGB와 마찬가지로 결정트리의 앙상블. 차이점은 RF는 Bagging을 통해 트리를 연결, XGB는 Boosting을 통해 트리를 연결

앙상블 방법은 크게 2가지

1. VotingClassifier처럼 사용자가 선택한 여러 종류의 머신러닝 모델을 연결하는 방식
2. XGBoost나 RF처럼 같은 종류의 모델을 여러개 합치는 앙상블

Bagging = Bootstrap Aggregation

Bootstraping: 중복을 허용한 샘플링

원본데이터 -> 부트스트래핑(샘플링) -> 애그리게이팅

중복허용한 샘플링을 통해 원래 가방에 있는 것보다 더 많은 샘플링이 가능함!

RF는 원본 데이터셋과 같은 크기의 부트스트래핑 샘플을 사용해 각 트리를 만듦. 수학적으로 평균적으로 각 트리의 샘플은 전체 샘플의 2/3 정도만 사용함.

분류일 경우, 다수결. 회귀일 경우 평균.

RF의 `n_estimator` 인자가 트리 개수

기본적으로 RF Classifier는 노드를 분할할 때 특성 개수의 제곱근을 사용함 => 중복 샘플을 가진 두 트리의 분할이 매우 달라져 매우 다른 예측을 만들게 됨. 분산을 줄이는 포인트

RF회귀모델은 부트스트랩샘플을 이용하지만 노드 분할에 특성의 제곱근이 아니라 feature를 전부 사용함

`oob_score` : True이면 각 트리에서 훈련시 사용되지 않은 샘플을 사용해 개별 트리의 예측 점수를 누적하여 평균을 냄

트리의 개수가 많아야 더 많은 oob sample이 나오게 됨. 당연히 각 개별 트리는 각각 다른 oob 샘플을 갖게됨

`warm_start`: 해당 매개변수는 트리 개수를 결정하는데 도움이 됨. True 시 이전 모델에 이어서 트리를 추가하게 됨

seaborn의 set()메서드는 set_theme()의 별칭. style 매개변수의 기본값이 dark grid 임

RF는 일반적으로 부트스트래핑을 사용하지만 boostrap 매개변수를 False로 지정할 수도 있음. 이 경우 oob_score_ 속성이 만들어지지 않음

verbose 매개변수 사용 시 유용한 정보를 얻을 수 있음

RF 구조 자체가 분산을 줄이도록 고안되었기 때문에 결정트리 매개변수가 RF에서 아주 중요하지는 않음

교차검증함수는 훈련된 모델을 반환하지 않기 때문에 oob_score_ 속성을 사용할 수 없음

CV 중 특정 셋만 값이 튄다면 shuffle을 통해 해결 가능할 수 있음

RF의 단점: 개별 트리에 제약이 된다. 모든 트리가 동일한 실수를 저지르면 RF도 실수를 저지름. 개별 트리가 해결할 수 없는 데이터 내의 문제 때문에 RF의 성능이 향상될 수 없었음

부스팅은 트리가 저지른 실수에서 배우도록 설계됨



## Ch4. From GB to XGB

부스팅의 기본 아이디어: 이전 트리의 오차를 기반으로 새로운 트리를 훈련. 즉 개별 트리가 이전 트리를 기반으로 만들어짐

GB의 새로운 트리는 올바르게 예측된 값에는 영향을 받지 않음. 따라서 잔차를 활용.

GB 모델 만들기

1. 결정트리 훈련. (기본학습기)
2. 훈련 세트에 대한 예측 수행
3. 잔차 계산(잔차는 다음 트리의 타깃이 됨)
4. 새로운 트리를 잔차에 대해 훈련
5. 2~4를 반복. 앙상블에 추가할 트리 개수만큼 반복이 계속됨
6. 각 트리의 테스트셋 예측 결과를 더함

learning rate는 개별 트리의 기여를 결정함. 일반적으로 트리 개수를 늘리면 lr을 줄여야 함. 따라서 최적의 lr은 트리 개수에 따라 다름

GBRegressor의 기본학습기는 DT임. 이 기본학습기의 매개변수도 조정할 수 있음

subsample 매개변수는 기본 학습기에 사용될 샘플의 비율을 지정

subsample이 1보다 작을 때 이런 모델을 '확률적 그레이디언트 부스팅'이라고 부름

subsample이 1보다 작으면 훈련에 사용되지 않은 샘플을 사용해 OOB 점수를 계산할 수 있음

XGBoost는 일반적인 구조는 동일한 GB의 고급 버전. 잔차로부터 훈련한 트리를 추가하여 약한 학습기를 강력한 학습기로 바꿈(learning_rate -> eta로 매개변수 변경)

XGB가 일반적으로 GB보다 선호되는 이유는 더 좋은 성능을 내면서, 더 빠르다는 것 때문

주피터 노트북은 %로 시작하는 매직함수를 제공함. %timeit도 매직함수 중 하나



## Ch5. XGBoost introduction

XGB는 계산능력을 극대화한 것. 모델 구축 뿐만 아니라 디스크 IO, 압축, 캐싱, CPU에 대한 지식도 필요함

XGB는 자체적으로 누락된 값을 처리할 수 있음. missing 매개변수에 어떤 값을 지정할 수 있음.
누락된 값이 있을 때 XGB는 가능한 노드 분할마다 점수를 매겨서 최상의 결과를 내는 분할을 선택함.

XGB가 속도 향상을 위해 고려한 것들

- 근사 분할 탐색 알고리즘
- 희소성 고려 분할 탐색
- 병렬 컴퓨팅
- 캐시 고려 접근
- 블록 압축과 샤딩

### 근사 분할 탐색 알고리즘

결정트리의 분할은 일반적으로 그리디 알고리즘으로 수행됨. 그러나 XGB는 새로운 근사 분할 탐색 알고리즘을 제공함.

분할 탐색 알고리즘은 데이터를 나누는 퍼센트인 분위수를 사용하여 후보 분할을 제안함. 전역제안에서는 동일한 분위수가 전체 훈련에 사용됨. 지역 제안에서는 각 분할마다 새로운 분위수를 제안함.

퀀타일 스케치 알고리즘은 가중치가 균일한 데이터셋에서 잘 동작함. XGB는 이론적으로 보장된 병합과 가지치기를 기반으로 한 새로운 가중 콴타일 스케치를 사용함

### 희소성 고려 분할 탐색

희소 데이터는 대부분의 원소가 0이거나 null인 데이터. 희소 행렬은 0이나 null이 아닌 데이터 포인트만 저장하여 공간을 절약함. 희소성 고려 분할은 분할을 탐색할 때 희소한 행렬에서 XGB가 더 빠르다는 것을 의미함.

### 병렬 컴퓨팅

부스팅은 각 트리가 이전 트리의 결과에 의존하기 때문에 병렬 컴퓨팅에 이상적이지 않음. 그러나 병렬화가 가능한 요소가 있음.

XGB는 데이터를 블록이라는 단위로 정렬하고 압축함. 이런 블록은 여러 대의 머신이나 외부 메모리에 분산될 수 있음.

블록을 사용하면 데이터를 더 빠르게 정렬할 수 있음. 분할 탐색 알고리즘은 블록의 장점을 사용해 분위수 탐색을 빠르게 수행함. 이런 요소에 병렬 컴퓨팅을 적용하여 모델 구축 과정의 속도를 높일 수 있음.

### 캐시 고려 접근

XGB는 캐시를 고려한 prefetching을 사용함. 내부 버퍼를 할당하고 그레이디언트 통계를 가져와 미니배치 방식으로 누적을 수행함.

### 블록 압축과 샤딩

블록 압축: 열을 압축하여 계산 비용이 많이 드는 디스크 읽기에 도움이 됨

블록 샤딩: 번갈아 가며 여러 디스크로 데이터를 샤딩하기 대문에 데이터를 읽는 시간을 줄여줌

----

XGB는 GB의 규제버전으로 볼 수도 있음. 목적함수의 일부로 규제를 포함하고 있기 때문.(대부분의 트리 앙상블과 XGB가 다른 점)

XGB의 목적함수 = 손실 함수 + 규제 항

XGBoost가 테일러 급수를 사용하는 방법에 대한 설명 https://stats.stackexchange.com/questions/202858/xgboost-loss-function-approximation-with-taylor-expansion

XGB의 규제항 = 리프 개수 + 리프노드의 점수 벡터 제곱의 합

사이킷런 API 존재 이전에는 XGB의 모델 구축 러닝커브가 매우 가팔랐음

`np.c_` 함수를 통해 두 넘파이 배열을 합칠 수 있음

XGBClassifier의 objective 매개변수 기본값: 다중분류일 경우 'multi:softprob', 이진분류일 경우 'binary:logistic'

RMSE와 타깃의 표준편차를 비교하여 모델 적합성을 대강 파악할 수도 있음

pd.read_csv에서 압축을 바로 풀어줄 수 있음

판다스는 파이썬의 del 키워드보다 drop() 메서드를 권장함

타깃 데이터 간 클래스 불균형 문제 해결은 매우 중요한 문제

사이킷런 API는 입력 데이터를 자동으로 DMatrix로 변환함. DMatrix는 속도를 위해 XGBoost가 최적화한 희소행렬임