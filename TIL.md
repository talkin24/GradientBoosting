# Part1. Bagging & Boosting

## Ch1. Gradient Boosting in ML

XGBoost는 표 형식 데이터를 활용한 예측에서 가장 뛰어난 머신러닝 알고리즘이다.

부스팅의 일반적인 아이디어: 약한 학습기(weak learner)를 반복적으로 오차를 개선하여 강한 학습기로 바꾸는 것

Gradient Boosting의 핵심 아이디어: 경사 하강법을 사용해 잔여 오차를 최소화 하는 것

`df_bikes.isna().any(axis=1)`: 열을 따라 누락된 값이 하나 이상인 모든 행을 찾음

`df_bikes.iloc[[56, 81]]`: 두개 이상의 인덱스를 찾을 때는 대괄호 두번 사용

`df_bikes.groupby('season')['hum'].transform('median')`: transform()메서드는 첫번째 매개 변수로 전달된 함수를 적용한 다음 원본과 동일한 길이의 시리즈나 데이터프레임을 반환함

`df_bikes['dteday'].apply(pd.to_datetime, infer_datetime_format=True, errors='coerce')`: infer_datetime_format=True로 지정하면 판다스가 datetime 객체의 종류를 결정하며, 대부분의 경우 안전함

`df_bikes.loc[730, 'yr'] = 1.0`loc 메서드를 사용하여 행과 열을 지정하여 특정 값을 변경할 수 있음

`fillna` 메서드에 원본과 동일한 길이의 시리즈 객체를 전달하면 누락된 위치에 있는 값만 채우는데 사용함

사이킷런의 `cross_val_score` 함수의 `scoring='neg_mean_squared_error'` 라고 쓰는 이유: 사이킷런은 점수가 높은것을 좋은 것으로 판단하기 때문

`df.info()` 는 메모리 사용량도 확인시켜줌



## Ch2. Decision Tree

gini impurity는 낮을수록 분류가 잘 된 것. 0.5인 경우 무작위 추측보다 더 낫다고 볼 수 없음

딱 한번만 분할된 트리는 stump라고 부른다. 이 스텀프도 부스터로 사용되면 강력해질 수 있음

훈련된 reg 객체의 tree_ 속성에 훈련된 트리 객체가 저장되어 있음

​	children_left, children_right 속성은 자식 노드의 인덱스를 담고 있음. 따라서 이들이 -1이면 리프노드라는 의미

GridSearchCV의 핵심은 매개변수 값의 딕셔너리를 만드는 것

`max_features` 옵션

- 0~1: 전체 feature의 비율
- 정수: 사용할 feature 개수
- None, auto: 전체 feature
- sort: 전체 feature의 제곱근
- log2: 전체 feature의 로그(밑이2)

`splitter` 옵션을 'random'으로 하면 과대적합을 막고 다양한 트리를 만들 수 있음. 'best' 옵션은 정보 이득이 가장 큰 특성만을 선택함

회귀와 분류의 criterion은 다름

- 회귀: squared_error, friedman_mse, absolute_error, poisson
- 분류: gini, entropy

`RandomizedSearchCV` 는 모든 조합을 테스트하는 대신 랜덤한 조합을 테스트함. 제한된 시간 내에 최적의 조합을 찾게 해줌. 또한 분포를 이용하여 연속적 매개변수 샘플링 가능. randint, uniform, loguniform 등

모델을 결정한 후에는 테스트셋을 포함하여 모델을 훈련하는것이 정확도를 더 높일 수 있음. 단 모델을 실전에 투여했을 때 얻을 수 있는 성능을 추정하기 어려워짐

`operator.itemgetter` 는 sorted 같은 함수의 key 매개변수에 적용하여 다양한 기준으로 정렬할 수 있도록 하는 모듈

사이킷런에서 추천하는 특성 중요도 측정 방법은 `permutation_importance()`함수임. 이 함수는 기존 모델에 특성 하나를 랜덤하게 섞은 후 모델을 훈련하여 성능을 비교



## Ch3. Bagging & Randomforest

RF는 XGB와 마찬가지로 결정트리의 앙상블. 차이점은 RF는 Bagging을 통해 트리를 연결, XGB는 Boosting을 통해 트리를 연결

앙상블 방법은 크게 2가지

1. VotingClassifier처럼 사용자가 선택한 여러 종류의 머신러닝 모델을 연결하는 방식
2. XGBoost나 RF처럼 같은 종류의 모델을 여러개 합치는 앙상블

Bagging = Bootstrap Aggregation

Bootstraping: 중복을 허용한 샘플링

원본데이터 -> 부트스트래핑(샘플링) -> 애그리게이팅

중복허용한 샘플링을 통해 원래 가방에 있는 것보다 더 많은 샘플링이 가능함!

RF는 원본 데이터셋과 같은 크기의 부트스트래핑 샘플을 사용해 각 트리를 만듦. 수학적으로 평균적으로 각 트리의 샘플은 전체 샘플의 2/3 정도만 사용함.

분류일 경우, 다수결. 회귀일 경우 평균.

RF의 `n_estimator` 인자가 트리 개수

기본적으로 RF Classifier는 노드를 분할할 때 특성 개수의 제곱근을 사용함 => 중복 샘플을 가진 두 트리의 분할이 매우 달라져 매우 다른 예측을 만들게 됨. 분산을 줄이는 포인트

RF회귀모델은 부트스트랩샘플을 이용하지만 노드 분할에 특성의 제곱근이 아니라 feature를 전부 사용함

`oob_score` : True이면 각 트리에서 훈련시 사용되지 않은 샘플을 사용해 개별 트리의 예측 점수를 누적하여 평균을 냄

트리의 개수가 많아야 더 많은 oob sample이 나오게 됨. 당연히 각 개별 트리는 각각 다른 oob 샘플을 갖게됨

`warm_start`: 해당 매개변수는 트리 개수를 결정하는데 도움이 됨. True 시 이전 모델에 이어서 트리를 추가하게 됨

seaborn의 set()메서드는 set_theme()의 별칭. style 매개변수의 기본값이 dark grid 임

RF는 일반적으로 부트스트래핑을 사용하지만 boostrap 매개변수를 False로 지정할 수도 있음. 이 경우 oob_score_ 속성이 만들어지지 않음

verbose 매개변수 사용 시 유용한 정보를 얻을 수 있음

RF 구조 자체가 분산을 줄이도록 고안되었기 때문에 결정트리 매개변수가 RF에서 아주 중요하지는 않음

교차검증함수는 훈련된 모델을 반환하지 않기 때문에 oob_score_ 속성을 사용할 수 없음

CV 중 특정 셋만 값이 튄다면 shuffle을 통해 해결 가능할 수 있음

RF의 단점: 개별 트리에 제약이 된다. 모든 트리가 동일한 실수를 저지르면 RF도 실수를 저지름. 개별 트리가 해결할 수 없는 데이터 내의 문제 때문에 RF의 성능이 향상될 수 없었음

부스팅은 트리가 저지른 실수에서 배우도록 설계됨



## Ch4. From GB to XGB

부스팅의 기본 아이디어: 이전 트리의 오차를 기반으로 새로운 트리를 훈련. 즉 개별 트리가 이전 트리를 기반으로 만들어짐

GB의 새로운 트리는 올바르게 예측된 값에는 영향을 받지 않음. 따라서 잔차를 활용.

GB 모델 만들기

1. 결정트리 훈련. (기본학습기)
2. 훈련 세트에 대한 예측 수행
3. 잔차 계산(잔차는 다음 트리의 타깃이 됨)
4. 새로운 트리를 잔차에 대해 훈련
5. 2~4를 반복. 앙상블에 추가할 트리 개수만큼 반복이 계속됨
6. 각 트리의 테스트셋 예측 결과를 더함

learning rate는 개별 트리의 기여를 결정함. 일반적으로 트리 개수를 늘리면 lr을 줄여야 함. 따라서 최적의 lr은 트리 개수에 따라 다름

GBRegressor의 기본학습기는 DT임. 이 기본학습기의 매개변수도 조정할 수 있음

subsample 매개변수는 기본 학습기에 사용될 샘플의 비율을 지정

subsample이 1보다 작을 때 이런 모델을 '확률적 그레이디언트 부스팅'이라고 부름

subsample이 1보다 작으면 훈련에 사용되지 않은 샘플을 사용해 OOB 점수를 계산할 수 있음

XGBoost는 일반적인 구조는 동일한 GB의 고급 버전. 잔차로부터 훈련한 트리를 추가하여 약한 학습기를 강력한 학습기로 바꿈(learning_rate -> eta로 매개변수 변경)

XGB가 일반적으로 GB보다 선호되는 이유는 더 좋은 성능을 내면서, 더 빠르다는 것 때문

주피터 노트북은 %로 시작하는 매직함수를 제공함. %timeit도 매직함수 중 하나



## Ch5. XGBoost introduction

XGB는 계산능력을 극대화한 것. 모델 구축 뿐만 아니라 디스크 IO, 압축, 캐싱, CPU에 대한 지식도 필요함

XGB는 자체적으로 누락된 값을 처리할 수 있음. missing 매개변수에 어떤 값을 지정할 수 있음.
누락된 값이 있을 때 XGB는 가능한 노드 분할마다 점수를 매겨서 최상의 결과를 내는 분할을 선택함.

XGB가 속도 향상을 위해 고려한 것들

- 근사 분할 탐색 알고리즘
- 희소성 고려 분할 탐색
- 병렬 컴퓨팅
- 캐시 고려 접근
- 블록 압축과 샤딩

### 근사 분할 탐색 알고리즘

결정트리의 분할은 일반적으로 그리디 알고리즘으로 수행됨. 그러나 XGB는 새로운 근사 분할 탐색 알고리즘을 제공함.

분할 탐색 알고리즘은 데이터를 나누는 퍼센트인 분위수를 사용하여 후보 분할을 제안함. 전역제안에서는 동일한 분위수가 전체 훈련에 사용됨. 지역 제안에서는 각 분할마다 새로운 분위수를 제안함.

퀀타일 스케치 알고리즘은 가중치가 균일한 데이터셋에서 잘 동작함. XGB는 이론적으로 보장된 병합과 가지치기를 기반으로 한 새로운 가중 콴타일 스케치를 사용함

### 희소성 고려 분할 탐색

희소 데이터는 대부분의 원소가 0이거나 null인 데이터. 희소 행렬은 0이나 null이 아닌 데이터 포인트만 저장하여 공간을 절약함. 희소성 고려 분할은 분할을 탐색할 때 희소한 행렬에서 XGB가 더 빠르다는 것을 의미함.

### 병렬 컴퓨팅

부스팅은 각 트리가 이전 트리의 결과에 의존하기 때문에 병렬 컴퓨팅에 이상적이지 않음. 그러나 병렬화가 가능한 요소가 있음.

XGB는 데이터를 블록이라는 단위로 정렬하고 압축함. 이런 블록은 여러 대의 머신이나 외부 메모리에 분산될 수 있음.

블록을 사용하면 데이터를 더 빠르게 정렬할 수 있음. 분할 탐색 알고리즘은 블록의 장점을 사용해 분위수 탐색을 빠르게 수행함. 이런 요소에 병렬 컴퓨팅을 적용하여 모델 구축 과정의 속도를 높일 수 있음.

### 캐시 고려 접근

XGB는 캐시를 고려한 prefetching을 사용함. 내부 버퍼를 할당하고 그레이디언트 통계를 가져와 미니배치 방식으로 누적을 수행함.

### 블록 압축과 샤딩

블록 압축: 열을 압축하여 계산 비용이 많이 드는 디스크 읽기에 도움이 됨

블록 샤딩: 번갈아 가며 여러 디스크로 데이터를 샤딩하기 대문에 데이터를 읽는 시간을 줄여줌

----

XGB는 GB의 규제버전으로 볼 수도 있음. 목적함수의 일부로 규제를 포함하고 있기 때문.(대부분의 트리 앙상블과 XGB가 다른 점)

XGB의 목적함수 = 손실 함수 + 규제 항

XGBoost가 테일러 급수를 사용하는 방법에 대한 설명 https://stats.stackexchange.com/questions/202858/xgboost-loss-function-approximation-with-taylor-expansion

XGB의 규제항 = 리프 개수 + 리프노드의 점수 벡터 제곱의 합

사이킷런 API 존재 이전에는 XGB의 모델 구축 러닝커브가 매우 가팔랐음

`np.c_` 함수를 통해 두 넘파이 배열을 합칠 수 있음

XGBClassifier의 objective 매개변수 기본값: 다중분류일 경우 'multi:softprob', 이진분류일 경우 'binary:logistic'

RMSE와 타깃의 표준편차를 비교하여 모델 적합성을 대강 파악할 수도 있음

pd.read_csv에서 압축을 바로 풀어줄 수 있음

판다스는 파이썬의 del 키워드보다 drop() 메서드를 권장함

타깃 데이터 간 클래스 불균형 문제 해결은 매우 중요한 문제

사이킷런 API는 입력 데이터를 자동으로 DMatrix로 변환함. DMatrix는 속도를 위해 XGBoost가 최적화한 희소행렬임



## Ch6. XGBoost Hyper-parameter

StratifiedKFold(계층적 분할기): 각 폴드의 타깃 레이블의 비율을 동일하게 만듦

cross_val_score() 함수와 GridSearchCV, RandomizedSearchCV에서 분류 모델을 받으면 기본적으로 StratifiedKFold 객체를 사용하기 때문에 cv 매개변수에 폴드수만 지정해도 됨

train_test_split() 함수는 기본적으로 데이터를 섞어서 나눔(shuffle=True)

모델의 하이퍼파라미터는 같은데 폴드의 차이로 점수가 다른 경우, 점수의 차이는 모델의 우위를 말할 수 없음

gamma 매개변수는 라그랑주 승수라고도 부름. 노드 분할을 위한 최소 손실 감소를 지정

조기종료를 통해 훈련 횟수를 제한할 수 있음. 조기 종료는 사전에 정한 훈련 반복 횟수를 채우지 않더라도, 연속적인 n번의 반복횟수 동안에 모델이 향상되지 않으면 훈련을 중지함.

early_stopping_rounds는 하이퍼파라미터라기 보다 n_estimators를 최적화 하기 위한 전략에 가까움

일반적인 하이퍼파라미터는 모든 부스팅 단계가 완료된 후 모델의 성능을 계산함. 그러나 조기종료를 사용하려면 매 반복마다 계산해야함. 따라서 fit메서드에 eval_metric(분류는 error, 회귀는 rmse)과 eval_set을 사용해야 함.

조기 종료는 목표치가 명확치 않은 대규모 데이터셋에서 유용함



## Ch7. XGBoost - exoplanet

불균형한 데이터셋을 다루는 데 익숙해지는 것이 모든 머신러닝 기술자의 필수사항

`sns.set` 은 밝은 회색 배경에 흰색 그리드를 만듦

시계열데이터라 하여도 다음 시계열을 예측하는 것이 아니면 분류모델로 활용할 수 있음

sklearn의  `classification_replort` 는 오차행렬에 담긴 숫자를 바탕으로 다양한 비율을 제공함.

- **precision: 양성 샘플로 예측한 것이 모두 맞아야할 때 이상적임**
- **recall: 양성 샘플을 모두 찾는 것이 목표일 때 잘 맞음**
- F1 Score: 정밀도와 재현율의 조화평균, 정밀도와 재현율 모두 중요할 때 사용, 0~1 사이
- support: 각 행에 해당하는 샘플 개수
- macro avg: 첫번째 행과 두번째 행의 평균
- weighted avg: 첫번째 행과 두번째 행의 가중 평균(support가 가중치)

오버샘플링을 먼저 하고 train/test set을 분리하면, 복사본이 양쪽 모두 들어가게 됨. 따라서 먼서 세트를 나누고 오버샘플링을 진행해야 함

`np.repeat()` 를 통해 넘파이배열을 복사할 수 있음

`SMOTE` 는 imblearn에서 임포트할 수 있는 인기있는 리샘플링 방법임

사이킷런은 타깃 값 1을 양성으로, 0을 음성으로 간주함(타깃값 변경이 필요할 수 있음)

직접 구현한 오버샘플링 기법과 `scale_pos_weight` 매개변수를 사용하여 만든 XGBClassifier는 동일한 예측을 만듦

GridSearch와 RandomizedSearch에서 일관된 결과를 얻기 위해, StratifiedKFold를 미리 사용할 수 있음

양섬샘플이 적을때는 어떤 샘플이 훈련 세트와 테스트 세트에 포함되는지가 차이를 만듦

`max_delta_step` 매개변수는 불균형한 데이터셋에서만 권장됨. 노드의 가중치에 대한 절댓값 한계를 설정함으로써 해당 매개변수 값이 증가할 수록 더 보수적인 모델이 만들어짐

전체 데이터를 사용한 점수가 낮고 오랜 시간이 걸린다면 생기는 질문. 작은 서브셋에서 머신러닝 모델이 더 잘 동작할까? 그렇지 않은 경우가 대부분 일 것

많은 사용자가 매우 불균형한 데이터에서 높은 정확도는 얻기 쉽고 사실상 의미 없다는 것을 이해하지 못함



## Ch8. XGBoost - Basic learner

기본학습기는 부스팅 단계마다 반복적으로 사용되는 개별 모델. 가장 많이 사용되는 것은 트리.

트리 외에도 gblinear(선형모델), dart(드롭 아웃 추가된 결정 트리) 등을 기본학습기로 사용할 수 있음

#### gblinear

결정트리는 비선형 데이터에 최적. 하지만 선형 모델이 적합한 경우도 있음. 이런 경우 gblinear를 사용하는 것이 좋음

일반적인 아이디어는 트리 부스팅과 동일. 기본 모델을 만들고 이어지는 후속 모델이 잔차를 바탕으로 훈련됨

gblinear를 여러번 부스팅하면 하나의 라소 회귀가 된다...!

로지스틱 회귀로 분류 문제에 사용할 수도 있음

선형 모델에 잘 맞는 실제 데이터셋은 드묾. 실제 데이터셋은 깨끗하지 않고 트리 앙상블과 같은 복잡한 모델이 더 나은 결과를 냄

StratifiedKFold는 target이 연속적이어서 클래스가 존재하지 않으면 사용할 수 없음. 대신 KFold 분할기를 사용해 클래스 비율을 고려하지 않고 일정하게 분할해야 함

gblinear 매개변수는 gbtree와 다름

- reg_lambda: L2 규제 크기
- reg_alpha: L1 규제 크기
- updater: 부스팅 단계마다 선형모델을 훈련하기 위해 사용하는 알고리즘. shotgun(좌표경사하강법), coord_descent(일반 경사 하강법)
- feature_selector: 좌표경사하강법 가중치 업데이트 단계에서 특성의 순서를 선택하는 방법
  - cyclic: 특성을 순환하면서 선택
  - shuffle: cyclic과 비슷하지만 업데이트 전에 랜덤하게 섞음
  - random: 완전 랜덤. 중복 선택 가능
  - greedy: 그레이디언트가 가장 큰 특성을 선택. 속도가 느림. 계산 비용이 비싸나 top_k 매개변수를 바꾸어 고려할 특성 개수를 줄일 수 있음
  - thrifty: 가중치 업데이트 전에 그레이디언트 크기에 따라 특성을 정렬
  - updater와 조합
    - shotgun: cyclic, shuffle
    - coord_descent: random, greedy, thrifty
- tok_k: greedy와 thrifty 방식에서 선택하는 최상위 특성 개수. 기본값: 0(모든 특성)

gblinear인 경우, 모델의 계수와 절편이 `coef_`, `intercept_` 속성에 저장됨

gblinear는 강력한 옵션이지만 선형 모델이 트리 기반 모델보다 더 높은 성능을 낼 수 있다는 확신이 있을 때만 사용해야 함. 데이터가 크고 선형적일때 좋은 선택임

#### DART

신경망에서 사용하는 드롭아웃 기법을 사용

모든 이전 트리의 잔차를 더하지 않고, 이전 트리를 랜덤하게 선택하고 1/k 배율로 리프 노드를 정규화 함. k는 드롭아웃된 트리의 개수

DART는 드롭아웃을 위해 추가 매개변수가 있는 gbtree라고 볼 수 있음

추가 매개변수: `sample_type, normalize_type,  rate_drop, one_drop, skip_drop`

dart는 XGB 프레임워크의 강력한 옵션. gbtree의 매개변수를 모두 이용할 수 있기 때문에 기본학습기를 dart로 쉽게 변환 가능

#### RF

`num_parallel_tree` 매개변수를 1보다 큰 값으로 설정하여 RF를 기본학습기로 사용할 수 있음

GB는 RF 같이 강한 학습기가 아니라 비교적 약한 학습기의 오차를 향상시키기 위해 고안됨. 그럼에도 불구하고 RF 기본 학습기가 도움이 될 수 있는 예외적인 경우가 있을 수 있음

XGBoost는 XGBRFRegressor, XGBRFClassifier를 제공함. 사이킷런의 RF와 비슷하나 XGB는 오버피팅을 방지하기 위한 기본 매개변수를 포함하고 있고 개별 트리를 만드는 방법이 다름

XGB에서 RF를 구현하는 방법은 2가지

1. RF를 기본학습기로 사용
   - RF기본학습기는 booster 매개변수에서 지정하지 않음. `num_parallel_tree` 매개변수를 1보다 크게 지정하면 gbtree를 부스팅 랜덤포레스트로 바꿈. 즉 부스팅 단계마다 하나의 트리가 아니라 여러개의 트리를 사용하여 앙상블을 구축함
2. XGBRFRegressor와 XGBRFClassifier를 사용하는 것
   - `num_parallel_tree` 가 아니라 `n_estimators` 를 사용. GB가 아니라 배깅 방식
   - `learning_rate` 는 한단계의 부스팅을 사용하는 이 모델들을 위한 것이 아니기 때문에 조정하지 않는 것이 좋음
   - `subsample, colsample_by_node` : 기본값이 0.8이므로 과대적합될 가능성이 낮음. 사이킷런의 랜덤 포레스트 구현과 큰 차이점.

RF를 기본 학습기로 사용하는 일은 많지 않음

XGB의 RF구현 모델들은 사이킷런의 RF 모델들 대신 사용할 수 있음





여러 조합을 dictionary로 만들어 비대칭 그리드 서치를 할 수 있음

사이킷런은 2차원 배열의 특성과 1차원 배열의 타깃을 기대함



## Ch09. Tips from Kaggle master

일반적으로 강력한 ML 모델을 만드는데 적용할 수 있는 기술

- 추가적인 홀드아웃 세트가 중요한 이유
- 평균 인코딩으로 새로운 특성을 만드는 Feature engineering
- VotingClassifier와 VotingRegressor를 사용해 상관관계가 낮은 머신러닝 앙상블을 만드는 방법
- 스태킹의 장점



캐글에서는 XGB나 LightGBM 같은 훌륭한 모델을 사용하는것으로 충분치 않음. 개별 모델의 예측이 중요하지만, 더 높은 성능을 달성하기 위해 새로운 특성을 만들고 최적의 모델을 결합하는 것도 중요함

많은 캐글러는 연구와 특성 공학(기존 특성에서 새로운 특성을 만드는 작업)에 상당한 시간을 사용한다고 말함. 문제는 특성 공학을 수행해야 하는지가 아니라, 특성 공학을 얼마나 많이 수행해야 하는지임

각 범주가 나타난 빈도로 범주형 특성을 변환할 수 있음

평균 인코딩: 범주형 특성을 타깃 값의 평균을 기반으로 수치 특성으로 변환
이는 필연적으로 데이터 누수가 있기 때문에 추가적인 규제가 필요. 그럼에도 불구하고 평균 인코딩은 놀라운 성능을 내는 것으로 입증됨. 실전에서 평균값의 분포가 비슷할 때 잘 동작함

평균인코딩이 원핫인코딩보다 반드시 좋다는 것은 아님. 다만 캐글 대회에서 잘 동작하는 입증된 기법이므로 시도해볼 가치가 있음

캐글 대회 우승 모델이 개별 모델인 경우는 드뭄. 거의 항상 앙상블을 사용함. 

좋은 앙상블을 위해서는 상관관계가 적은 모델을 고르는 것이 중요

앙상블과 스태킹은 비슷하나 스태킹이 더 장점이 많다,,,? 캐글 우승자들이 사용하는 가장 강력한 기법 중 하나

스태킹은 두개의 수준으로 모델을 결합

- 기본 수준: 모델이 모든 데이터를 사용하여 예측을 만듦
- 메타 수준: 베이스 모델의 예측을 입력으로 받아 최종 예측을 만듦. 즉 최종 모델은 원본 데이터를 입력으로 사용하지 않고, 베이스 모델의 예측을 입력으로 사용함
  메타 모델을 예측을 입력으로 받기 때문에 회귀에서는 선형회귀, 분류에서는 로지스틱 회귀와 같은 간단한 모델을 사용하는 것이 권장 됨

스태킹 역시 상관관계가 낮은 모델들을 사용하는 것이 권장됨



## Ch10. Deploy XGB

제품을 위한 모델 배포는 연구나 경연 대회를 위해 모델을 만드는 것과 조금 다름

수치형 특성의 누락된 값은 -999.0
범주형 특성의 누락된 값은 최빈값(분포에 영향을 미치지 않는 정도로). 또는 unknown 같은 문자열로 바꾸기

pd.get_dummies() 함수 단점

- 계산비용 큼
- 사이킷런 파이프라인에 통합되지 않음

=> OneHotEncoder 사용. 모든 범주형 특성을 0과 1로 인코딩하는건 같으나, 밀집배열 대신 희소 행렬을 사용하여 공간과 시간을 절약함. 희소행렬은 0이 아닌 값만 저장하여 공간을 절약함

특정 타입 열만 골라내기 `cold_df = df.select_dtypes(exclude=['object'])`

수치형을 희소행렬로 변환하거나, 원핫인코딩 출력을 일반 df로 변환하거나

